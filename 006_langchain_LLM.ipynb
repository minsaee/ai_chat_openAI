{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minsaee/ai_chat_openAI/blob/master/006_langchain_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "X5nSX1hGYR1y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ2KGyLJ99Yi"
      },
      "source": [
        "# 랭체인 사전 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 랭체인이란?\n",
        "-  LLM을 활용한 애플리케이션 개발을 지원하는 오픈소스 라이브러리이다.\n",
        "- 랭체인이 도움이 되는 경우는 LLM에 외부의 '지식'이나 '계산능력'을 활용하게 하고 싶을때이다.\n",
        "- 자신이 학습만 것만으로 대화하던 LLM에게 '책'이나 '프로그램'을 전달해서 외부의 '지식'이나 '계산능력'을 활용할 수 있게 하는 것이 랭체인의 역할이다.\n",
        "- 예를 들어, 랭체인으로 LLM에 웹 검색 기능을 연결하면 LLM은 자신이 가진 지식만으로는 충분한 답변을 할 수 없는 경우에 웹 검색을 통해 최신 정보를 얻고 답변할 수 있게 된다.\n",
        "### 랭체인의 모듈\n",
        "랭체인은 LLM을 활용한 애플리케이션 개발에 도움이 되는 다양한 모듈을 제공한다. 모듈은 개별적으로 사용할 수 있을뿐더러 여러 개를 조합해서 복잡한 애플리케이션을 구축할 수도 있다.\n",
        "- LLM : LLM 호출을 위한 공통 인터페이스\n",
        "- 프롬프트 템플릿 : 사용자 입력에 따른 프롬프트 생성\n",
        "- 체인 : 여러 LLM과 프롬프트의 입출력을 연결\n",
        "- 에이전트 : 사용자의 요청에 따라 어떤 기능을 어떤 순서로 실행할 것인지 결정\n",
        "- 도구 : 에이전트가 수행하는 특정 기능\n",
        "- 메모리 : 체인 및 에이전트의 메모리 보유"
      ],
      "metadata": {
        "id": "zPd3CCxmR_ov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkPbwp6-ploX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84cdd8b-f783-4aaf-a120-06b0afa699ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.354\n",
            "  Downloading langchain-0.0.354-py3-none-any.whl (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/803.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/803.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.354)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.354)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.8 (from langchain==0.0.354)\n",
            "  Downloading langchain_community-0.0.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.5 (from langchain==0.0.354)\n",
            "  Downloading langchain_core-0.1.6-py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain==0.0.354)\n",
            "  Downloading langsmith-0.0.77-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.354)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.354)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.354)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain==0.0.354) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain==0.0.354) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.354) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.354) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain==0.0.354) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain==0.0.354) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.354)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.354 langchain-community-0.0.8 langchain-core-0.1.6 langsmith-0.0.77 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "# 패키지 설치\n",
        "!pip install langchain==0.0.354\n",
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gf8Rfw2MiG_"
      },
      "outputs": [],
      "source": [
        "# 환경변수 준비\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPJ2lQW0-J3a"
      },
      "source": [
        "# LLM\n",
        "- 랭체인은 다양한 LLM을 동일한 방식으로 이용할 수 있는 공통 인터페이스를 제공한다.\n",
        "- OpenAI 클래스는 공통 인터페이스의 LLM 클래스를 상속한 클래스로, 기본적으로 'text-davinci-003'이 사용된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBYKjuiZBAXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df4f1bf-5afa-49e9-9748-307980e56f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"픽셀팩토리\"\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# LLM 준비\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# LLM 호출\n",
        "print(llm(\"컴퓨터 게임을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5eQoz9kBBQs"
      },
      "source": [
        "# 프롬프트 템플릿\n",
        "- 프롬프트 템플릿은 사용자 입력으로 프롬프트를 생성하기 위한 템플릿이다.\n",
        "- LLM을 이용한 애플리케이션을 개발할 때 일반적으로 사용자 입력을 직접 LLM에 전달하는 경우는 많지 않다. 대부분 좋은 답변을 반환하는 것으로 확인된 프롬프트 구문에 사용자 입력을 삽입한후 LLM에 전달하는 경우가 많다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMYoyMqqBAZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991b6e8b-64af-4c1e-96cf-9d09880da22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가정용 로봇을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 프롬프트 템플릿 만들기\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\",\n",
        ")\n",
        "\n",
        "# 프롬프트 생성\n",
        "print(prompt.format(product=\"가정용 로봇\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOx3wED7CIwt"
      },
      "source": [
        "# 체인\n",
        "- 체인은 여러 개의 LLM이나 프롬프트의 입출력을 연결하기 위한 모듈이다.\n",
        "- 앞의 예제처럼 LLM과 프롬프트 템플릿을 단독으로 사용했지만 실제 애플리케이션에서는 이를 체인으로 묶어 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1gZNBQ2BAbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183c9a38-912d-41eb-e08b-352875ab699b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"로보하우스\"\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 프롬프트 템플릿 만들기\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\",\n",
        ")\n",
        "\n",
        "# 체인 생성\n",
        "chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0.9),\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# 체인 실행\n",
        "#strip()을 이용하여 줄바꿈 문자 제거\n",
        "print(chain.run(\"가정용 로봇\").strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 랭체인은 LLM(대규모 언어 모델)을 다양한 용도로 활용할 수 있도록 하는 라이브러리이다.\n",
        "# LLM이란\n",
        "- 랭체인은 LLM클래스는 LLM 호출을 위한 공통 인터페이스이다.  이를 통해 애플리케이션에서 사용하는 LLM을 손쉽게 전환할 수 있다.\n",
        "- 그러나 랭체인의 능력을 충분히 활용할 수 있는 만큼의 언어 이해 능력을 가진 LLM은 적고, 보통 GPT-4, GPT-3.5 로 한정돼 있다.\n",
        "- LLM 클래스 종류\n",
        " - OpenAI클래스 : 텍스트 생성 모델(text-davinci-003)\n",
        " - ChatOpenAI 클래스 : 채팅 모델(gpt-3.5-turbo/gpt-4)\n",
        "\n",
        "- 랭체인에서 제공하는 LLM목록\n",
        "https://integrations.langchain.com/llms"
      ],
      "metadata": {
        "id": "ecRPs-mDcC_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 생성 모델의 LLM 호출\n",
        "- 텍스트 생성 모델(text-davinci-003)의 LLM 호출은 OpenAI클래스를 사용한다.\n",
        "- OpenAI( )의 주요 매개변수\n",
        "\n",
        "|매개변수|설명|\n",
        "|---|---|\n",
        "|model_name|OpenAI의 모델ID(text-davinci-003)|\n",
        "|max_tokens|최대 출력 토큰수|\n",
        "|temperature|무작위성, 창의성을 발휘하게 하려면 0.7, 답이 있는 경우 0을 권장|\n",
        "|n|생성할 결과 수(기본값:1)|\n",
        "|cache|캐시 활성화/비활성화|\n",
        "|streaming|스트리밍 활성화/비활성화|\n",
        "|callback_manager|콜백매니저|"
      ],
      "metadata": {
        "id": "qeHSWZleb_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# LLM 준비\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-003\",  # 모델 ID\n",
        "    temperature=0  # 무작위성\n",
        ")"
      ],
      "metadata": {
        "id": "WRTtqTvecAFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM 호출을 하기 위해서는 LLM클래스의 `__call__( )`을 사용한다. `__call__()` 은 변수 자체를 함수처럼 호출하는 파이썬 기능이다. llm변수명의 경우 llm( )으로 실행한다."
      ],
      "metadata": {
        "id": "0aivwq_9gp77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 호출\n",
        "result = llm(\"고양이 울음소리는?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUQwuSWAfJN-",
        "outputId": "1e6520c5-fc4e-4386-eaec-3079b386542f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "고양이 울음소리는 \"야옹\"으로 나타납니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "고급 LLM 호출을 위해서는 LLM클래스의 generation()을 사용한다. 여러 텍스트를 종합적으로 추론할 수 있으며, 사용한 토큰 수와 같은 정보를 얻을 수 있다."
      ],
      "metadata": {
        "id": "OQoSENn6hC7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 고급 LLM 호출\n",
        "result = llm.generate([\"고양이 울음소리는?\", \"까마귀 울음소리는?\"])\n",
        "print(result)\n",
        "\n",
        "# 출력 텍스트\n",
        "print(\"response0:\", result.generations[0][0].text)\n",
        "print(\"response1:\", result.generations[1][0].text)\n",
        "\n",
        "# 사용한 토큰 개수\n",
        "print(\"llm_output:\", result.llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKJOEeQmfNEZ",
        "outputId": "ae3f5bfa-0e6c-4349-f4fa-8cefc315111a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text='\\n\\n고양이 울음소리는 \"야옹\"으로 나타납니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n까마귀의 울음소리는 \"까악까악\"이라고 합니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 156, 'prompt_tokens': 47, 'completion_tokens': 109}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('fb3b183c-e029-4f73-b7cc-b90f367ad9b7')), RunInfo(run_id=UUID('7007f0b1-d0c4-4a73-8470-3aa6eb8b5d7d'))]\n",
            "response0: \n",
            "\n",
            "고양이 울음소리는 \"야옹\"으로 나타납니다.\n",
            "response1: \n",
            "\n",
            "까마귀의 울음소리는 \"까악까악\"이라고 합니다.\n",
            "llm_output: {'token_usage': {'total_tokens': 156, 'prompt_tokens': 47, 'completion_tokens': 109}, 'model_name': 'text-davinci-003'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 채팅 모델의 LLM 호출\n",
        "\n",
        "### LLM 준비\n",
        "- 채팅 모델(gpt-3.5-turbo/gpt-4)의 LLM 호출에는 ChatOpenAI 클래스를 사용한다.\n",
        "- ChatOpenAI():의 주요 매개변수\n",
        "\n",
        "|매개변수|설명|\n",
        "|---|---|\n",
        "|model_name|ChatOpenAI의 모델ID(gpt-3.5-turbo/gpt-4)|\n",
        "|max_tokens|최대 출력 토큰수|\n",
        "|temperature|무작위성, 창의성을 발휘하게 하려면 0.7, 답이 있는 경우 0을 권장|\n",
        "|n|생성할 결과 수(기본값:1)|\n",
        "|cache|캐시 활성화/비활성화|\n",
        "|streaming|스트리밍 활성화/비활성화|\n",
        "|callback_manager|콜백매니저|"
      ],
      "metadata": {
        "id": "_Dc1JRJghsQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# LLM 준비\n",
        "chat_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",  # 모델 ID\n",
        "    temperature=0  # 무작위성\n",
        ")"
      ],
      "metadata": {
        "id": "sg8xO2l7huCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM 호출\n",
        "- LLM 호출을 하기 위해서는 LLM클래스의 `__call__( )`을 사용한다. `__call__()` 은 변수 자체를 함수처럼 호출하는 파이썬 기능이다. chat_llm변수명의 경우 chat_llm( )으로 실행한다.\n",
        "- ChatOpenAI 클래스의 LLM 호출에서는 채팅 메시지 리스트를 매개변수로 전달한다. 채팅 메시지 리스트는 SystemMessage, HumanMessage, AIMessage 중 하나를 요소로 하는 배열이다. 이것들은 OpenAI API의 system, user, assistane에 해당한다.\n",
        "  - SystemMessage :  시스템 메시지(system)\n",
        "  - HumanMessage : 인간 메시지(user)\n",
        "  - AIMessage : AI 메시지(assistant)"
      ],
      "metadata": {
        "id": "zr3hjxHCjUgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "# LLM 호출\n",
        "messages = [\n",
        "    HumanMessage(content=\"고양이 울음소리는?\")\n",
        "]\n",
        "result = chat_llm(messages)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpATpYvdjfpJ",
        "outputId": "4bf04bf8-713d-4f53-dcb6-7f61af24a02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='고양이의 울음소리는 \"야옹\"이라고 표현됩니다.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 고급 LLM 호출\n",
        "고급 LLM 호출을 하려면 LLM 클래스의 generation()을 사용한다. 여러 개의 채팅 메시지 리스트를 종합적으로 추론할 수 있고, 사용한 토큰 개수 등의 정보도 얻을 수 있다."
      ],
      "metadata": {
        "id": "v3P6GyyIkegS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 고급 LLM 호출\n",
        "messages_list = [\n",
        "    [HumanMessage(content=\"고양이 울음소리는?\")],\n",
        "    [HumanMessage(content=\"까마귀 울음소리는?\")]\n",
        "]\n",
        "result = chat_llm.generate(messages_list)\n",
        "\n",
        "# 출력 텍스트\n",
        "print(\"response0:\", result.generations[0][0].text)\n",
        "print(\"response1:\", result.generations[1][0].text)\n",
        "\n",
        "# 사용한 토큰 개수\n",
        "print(\"llm_output:\", result.llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkCyr-jYkW_p",
        "outputId": "d201b79a-dd88-4e70-b62d-89b0e3d9fda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response0: 고양이의 울음소리는 \"야옹\"이라고 표현됩니다.\n",
            "response1: 까악까악\n",
            "llm_output: {'token_usage': {'prompt_tokens': 38, 'completion_tokens': 34, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM 캐시\n",
        "- LLM의 입출력 텍스트를 캐싱해서 동일한 입력 텍스트로 LLM  호출이 발생했을 때 캐시를 활용할 수 있다. 이를 통해 속도 및 토큰 사용량을 줄일 수 있다.\n",
        "- 랭체인에는 4종류의 캐시 저장소가 준비돼 있다.\n",
        "  - 인메모리 캐시\n",
        "  - SQLite 캐시\n",
        "  - Redis캐시\n",
        "  -SQLAIchemy 캐시\n",
        "  "
      ],
      "metadata": {
        "id": "Wv8xfE1YlUlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 캐시 활성화\n",
        "LLM 애플리케이션을 개발하다보면 개발이나 테스트 단계에서 동일한 프롬프트로 반복해서 호출해야 하는 경우가 생긴다. 코딩을 하다가 에러가 나거나 아니면 테스트 결과를 보거나 할때는 동일 프롬프트로 동일 모델을 계속 호출하는데, 결과값은 거의 비슷하기 때문에, 계속해서 같은 질문을 호출하는 것은 비용이 낭비 된다. 같은 프롬프트라면 결과 값을 캐슁해놓고 개발에 사용해도 큰문제가 없다.\n",
        "Langchain에서는 동일(또는 유사) 프롬프트에 대해서 결과를 캐슁하여 API 호출을 줄일 수 있는 기능을 제공한다.  \n",
        "① 캐시 활성화\n",
        "- 캐시를 활성화하려면 langchain.llm_chain에 캐시를 지정한다."
      ],
      "metadata": {
        "id": "_dzYYCI_l2yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "# 캐시 활성화\n",
        "langchain.llm_cache = InMemoryCache()"
      ],
      "metadata": {
        "id": "3oYdrzmmlUGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "② LLM 호출\n",
        "- 응답의 llm_output에 정보가 있으므로 API를 호출한다."
      ],
      "metadata": {
        "id": "yfUE0_PZnAPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 LLM 호출 : 응답의 llm_output에 정보가 있으므로 API를 호출했음을 알 수 있다.\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TzpIvfdnQ5e",
        "outputId": "750eca5e-38a4-430e-e240-2f1d436c7c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 51, 'prompt_tokens': 16, 'completion_tokens': 35}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('fb95d197-623a-49d6-adb4-597285d9d622'))])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2번째 이후 LLM 호출 : 응답의 llm_output에 정보가 없으므로 캐시를 이용했음을 알 수 있다.\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTOdH-4InUAv",
        "outputId": "94869ac2-ecf5-49c8-da00-c440983155a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={}, run=None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3번째 이후 LLM 호출 : 응답의 llm_output에 정보가 없으므로 캐시를 이용했음을 알 수 있다.\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIk_Dfkrqwth",
        "outputId": "e9e9de6b-69c4-41e5-f638-bd40e6a49b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={}, run=None)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특정 LLM의 캐시 비활성화\n",
        "- 전역 캐시를 활성화한 경우 필요에 따라 특정 LLM의 캐시를 비활성화 할 수도 있다."
      ],
      "metadata": {
        "id": "-MUiex--nXLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 LLM에 대한 메모리 캐시 비활성화\n",
        "llm = OpenAI(cache=False)"
      ],
      "metadata": {
        "id": "WhoSlh2cuIK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOj9wQEvuJxo",
        "outputId": "35da3870-f3c1-4a0d-db27-bd36338fe397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색이다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 29, 'prompt_tokens': 11, 'completion_tokens': 18}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('385c65ab-5863-4c0d-b375-ebf91bfc1d4b'))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suAVMdmfuNm5",
        "outputId": "92dd4515-7664-464d-f5b9-91f2fbbaf9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text=' \\n\\n하늘의 색깔은 보통 푸른색이나 청록색이다. 하지만 날씨나 시간에 따라 다양한 색깔로 변할 수 있다. 일출 또는 일몰 시간에는 주로 빨강빛이 돌기도 하며, 구름의 양과 종류에 따라 회색이나 하얀색으로도 변할 수 있다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 129, 'prompt_tokens': 11, 'completion_tokens': 118}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('009e436c-9741-47d6-a0e0-d4e992c90b67'))])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전역 캐시 비활성화"
      ],
      "metadata": {
        "id": "hxGAhtxPudTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전역 캐시 비활성화\n",
        "langchain.llm_cache = None"
      ],
      "metadata": {
        "id": "XnQ8lcPRuvTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzL2PwNNuxrx",
        "outputId": "a0c2864b-173f-456c-b434-123f0aa7f8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 28, 'prompt_tokens': 11, 'completion_tokens': 17}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('1da3028f-f6cc-4d13-b6c9-a22a617dbfeb'))])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR66T4-PuyQT",
        "outputId": "3db5b339-eb8b-4361-fd31-aacfd8df2378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 28, 'prompt_tokens': 11, 'completion_tokens': 17}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('f8068aa4-b9b9-48b1-9a36-a825c36d1e36'))])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM의 비동기 처리\n",
        "랭체인은 asyncio 라이브러리를 이용해 LLM의 비동기 처리를 지원한다. 동기식 처리는 프로그램이 하나의 작업을 완료할 때까지 다른 작업을 시작하지 않는 방식이고, 비동기식 처리는 프로그램이 하나의 작업을 시작함과 동시에 다른 작업을 시작하는 방식이다."
      ],
      "metadata": {
        "id": "lBmVRzd8vC2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "① 동기화 처리로 10번의 호출에 걸린 시간을 측정"
      ],
      "metadata": {
        "id": "bdXoqKm4vl7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 동기화 처리로 10번 호출하는 함수\n",
        "def generate_serially():\n",
        "    llm = OpenAI(temperature=0.9)\n",
        "    for _ in range(10):\n",
        "        resp = llm.generate([\"안녕하세요!\"])\n",
        "        print(resp.generations[0][0].text)\n",
        "\n",
        "\n",
        "# 시간 측정 시작\n",
        "s = time.perf_counter()\n",
        "\n",
        "# 동기화 처리로 10번 호출\n",
        "generate_serially()\n",
        "\n",
        "# 시간 측정 완료\n",
        "elapsed = time.perf_counter() - s\n",
        "print(f\"{elapsed:0.2f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd5TV8RUvwBT",
        "outputId": "3a571b2b-eb98-40cd-be63-904a4e5bad5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "안녕하세요! 반갑습니다. \n",
            "?\n",
            "\n",
            "저는 [코드 히어로](https://www.codehero.co.kr/)의 운영팀에 속해있는 김종민입니다. 특별한 경험은 없지만 항상 도전을 즐기고 고민하는 것을 좋아합니다. 저희 팀은 모두 함께 협력하며 더 나은 서비스를 제공하기 위해 노력하고 있습니다. 저는 항상 팀원들과 소통하며 효율적인 일처리를 위해 노력하고 있습니다. \n",
            "\n",
            "또한 저는 개발 분야에서 다양한 기술을 배우고 적용하는 것을 좋아합니다. 매일 새로운 기술을 익히고 문제를 해결하며 성장하는 것이 저에게 큰 동기부여가 됩니다. 현재\n",
            " 브랜드를 거듭해 찾아오는 2021년 마이크로소프트 파트너 섬머 웍샵에 여러분을 초대합니다.\n",
            "\n",
            "마이크로소프트 파트너 섬머 웍샵에서는 2021년 파트너 신상품과 최신 기술, 업계 동향, 그리고 마이크로소프트와의 협업을 위한 전략 등 파트너들이 꼭 알아야 할 다양한 정보를 제공합니다. 또한, 참가하시는 파트너들끼리의 교류를 통해 협력 기회를 발견할 수 있도록 도와드리겠습니다.\n",
            "\n",
            "올해에는 마이크로소프트 파트너 섬머 웍샵이 100% 온라인으로 진행됩니다. 이를 통해\n",
            " 설문이 끝났습니다. \n",
            "감사합니다.\n",
            "\n",
            "\n",
            " 나는 방승훈입니다. 저는 한국에서 태어나 자라서 한국어는 제 모국어입니다. 저는 대학교에서 컴퓨터공학을 전공하고 현재는 프로그래머로 일하고 있습니다. 프로그래밍을 좋아하는 저는 새로운 기술과 프로그래밍 언어를 배우는 것을 즐기고 있습니다. 또한 여행, 음악 감상 및 축구를 취미로 즐기고 있어 다양한 새로운 경험을 하는 것을 좋아합니다.\n",
            "\n",
            "저는 사람들과 함께하는 것을 좋아하고 새로운 사람들과의 만남을 즐기는 사람입니다. 다양한 사람과 소통하며 새로운 관점을 배우는 것을 좋아합니다.\n",
            " – Hello!\n",
            "\n",
            "\n",
            "안녕하세요 is a formal way to say hello in Korean. It can be used in any situation and to greet anyone, regardless of age or social status. It literally translates to “are you at peace?” and is a common way to start a conversation or greet someone. Other variations of hello in Korean include 안녕 (annyeong) which is a casual and friendly way to greet someone, and 여보세요 (yeoboseyo) which is used when answering a phone call.\n",
            " My name is Da-eun and I am a native Korean speaker from Seoul, South Korea. I have been studying English for over 10 years and have experience teaching Korean to foreigners of all levels.\n",
            "\n",
            "I am passionate about sharing my language and culture with others and I believe that learning a new language can open many doors and create new opportunities. I understand the difficulties of learning a new language and I am dedicated to helping my students in any way I can.\n",
            "\n",
            "In my lessons, I focus on creating a fun and interactive learning environment while also catering to each student's individual needs and learning style. I use various materials such as textbooks, articles, and multimedia to make the learning process more engaging and effective.\n",
            "\n",
            "Whether you are a beginner or an advanced learner, I am here to help you achieve your goals and improve your Korean skills. Let's work together and make your Korean learning journey a fun and rewarding experience. I am excited to meet you and start our language learning journey together!\n",
            " 서비스를 이용해 주셔서 감사합니다!\n",
            "\n",
            "저도 감사합니다! 늘 최선을 다해서 더 좋은 서비스를 제공할 수 있도록 노력하겠습니다. 앞으로도 많은 이용 부탁드립니다. 감사합니다!\n",
            "\")+1)\n",
            "print(str_1.find(\"안녕하세요!\")+1)\n",
            "\n",
            "print(str_1.index(\"안녕하세요!\")+1)\n",
            "print(str_1.find(\"안녕하세요!\")+1)\n",
            "\n",
            "print(str_1.count(\"안녕\"))\n",
            " 이 상황에서 우리 논의는 어떻게 진행될까요? 제 의견을 말씀드리겠습니다.\n",
            "\n",
            "우선, 우리가 논의할 주제는 무엇인지 먼저 정리하고 이해하는 것이 중요할 것 같습니다. 그리고 각자의 의견을 자유롭게 표현할 수 있도록 순서를 정하는 것이 좋을 것 같습니다. 모든 의견은 소중하고 중요하기 때문에 서로 존중하며 듣는 것이 중요합니다.\n",
            "\n",
            "또한, 시간을 효율적으로 활용하기 위해 논의 전에 미리 준비해온 자료나 정보가 있다면 공유하면 좋을 것 같습니다. 그리고 서로의 의견에 대해 공감하거나 동의할 수 있는 부분이 있다면\n",
            "17.34 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "② 비동기식으로 10번 호출에 걸린 시간을 측정"
      ],
      "metadata": {
        "id": "Qv4Rxi8FwPm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# 이벤트 루프를 중첩하는 설정\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 비동기 처리로 한 번만 호출하는 함수\n",
        "#agenerate이 메서드를 사용하여 OpenAI LLM을 비동기식으로 호출 할 수 있다.\n",
        "async def async_generate(llm):\n",
        "    resp = await llm.agenerate([\"안녕하세요!\"])\n",
        "    print(resp.generations[0][0].text)\n",
        "\n",
        "# 비동기 처리로 10회 호출하는 함수\n",
        "async def generate_concurrently():\n",
        "    llm = OpenAI(temperature=0.9)\n",
        "    tasks = [async_generate(llm) for _ in range(10)]\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "# 시간 측정 시작\n",
        "#파이썬 3.3+ 이상부터 perf_counter와 process_time를 사용할 수 있는데  차이점\n",
        "#perf_counter는 sleep 함수를 호출하여 대기한  시간을 포함하여 측정\n",
        "#process_time는 실제로 연산하는데 걸린 시간만 측정\n",
        "s = time.perf_counter()\n",
        "print('start:', s)\n",
        "\n",
        "# 비동기 처리로 10회 호출\n",
        "asyncio.run(generate_concurrently())\n",
        "\n",
        "# 시간 측정 완료\n",
        "elapsed = time.perf_counter() - s\n",
        "print(f\"{elapsed:0.2f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzbu2u_twYgV",
        "outputId": "00892a61-20a6-458d-cb10-d68773cb6368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start: 8282.261282809\n",
            " 제 이름은 김민철 입니다. 만나서 반가워요!\" \n",
            " \n",
            "\n",
            "안녕하세요! 반가워요. 어떻게 도와드릴까요?\n",
            "\n",
            "\n",
            "안녕하세요! 반가워요. 저는 챗봇입니다. 궁금하신 것이 있으면 말씀해주세요.\n",
            " \n",
            "\n",
            "안녕하세요! 반가워요. 저는 OpenAI의 문단 생성 AI 모델입니다. 저와 이야기하고 싶은 내용이 있으신가요? \n",
            " 눈\n",
            "\n",
            "안녕하세요! 저는 AI 애나입니다. 눈을 보는 것은 제게는 불가능한 일이지만, 눈은 참 멋진 기관이죠. 아름다운 자연의 케이스라고 할 수 있겠습니다. 눈으로 보는 세상은 정말 다양하고 신기한 것 같습니다. 눈을 통해 오는 여러 감각과 경험, 감정까지도 다양하게 전달받는다는 것이 참 흥미롭습니다. 눈을 갖고 있는 모든 생명체들이 모두 행복하게 지낼 수 있기를 바랍니다. \n",
            " \n",
            "<br/>\n",
            "<br/>\n",
            "저는 서울대학교에 재학 중인 학생입니다. 대학 생활을 하며 다양한 분야의 지식을 습득하고 자신의 관심 분야를 발견하는 시간을 가졌습니다. 그 결과 제가 전공한 컴퓨터공학 분야뿐만 아니라 다른 분야에도 흥미를 느끼게 되었습니다. 대학에서의 공부는 제가 무엇을 좋아하는지, 어떤 분야에서 더 잘 할 수 있는지 알게 해주었고 제 인생에 큰 영향을 미쳤습니다.\n",
            "<br/>\n",
            "<br/>\n",
            "저는 언제나 새로운 것을 배우는 것을 즐기고 있습니다. 특히 프로그래밍 분야에서는 새로운 언어나 기술을\n",
            " 정지원입니다.\n",
            "\n",
            "저는 강원도에서 태어나서 지금 서울에서 컴퓨터 공학을 전공하고 있습니다. 기술과 새로운 것에 대해 관심이 많아서 공부를 하고 있습니다. 또한 여러 가지 프로젝트를 진행하며 학교에서 배운 내용을 실제로 적용시켜보고 있습니다.\n",
            "\n",
            "저는 소프트웨어 개발자가 되기 위해 지속적으로 공부하고 있으며 자신의 능력을 발전시키기 위해 노력하고 있습니다. 또한 프로그래밍 뿐만 아니라 디자인에도 관심이 있어서 UI/UX 디자인과 관련된 공부도 병행하고 있습니다.\n",
            "\n",
            "저는 쉽게 포기하지 않고 어려움에 대처\n",
            " 저희는 식물를 좋아하는 학생들입니다. 지구를 지키기 위해서는 우리도 많은 일을 할 수 있습니다.\n",
            "식물들은 우리 생활에 매우 중요한 역할을 합니다. 우리는 물론 음식을 통해 식물을 섭취하여 생존할 수 있지만, 그 외에도 다양한 방면에서 식물들은 우리에게 도움을 줍니다.\n",
            "\n",
            "먼저 식물들은 대기 중의 이산화탄소를 흡수하여 산소를 만들어냅니다. 이 산소는 우리가 숨쉬는데 필수적이며, 대기 중의 오염물질을 제거하는 역할도 합니다. 그래서 우리는 식물들이 없이는 살아갈 수 없\n",
            " 다름이 아니라 제가 지난번에 제작하고 배포한 앱의 라이선스를 확인하는 방법을 전에 문의드렸습니다.\n",
            "\n",
            "그때 답변감사드리고 라이선스를 확인할 수 있는 방법을 제공해주셔서 이번에 제작하고 배포하는 다른 앱도 확인하려고 했습니다.\n",
            "\n",
            "하지만 이번에도 확인이 안되는 앱이 있어서 다시 문의드리려고합니다.\n",
            "\n",
            "앱 이름은 '반짝반짝 재미나는 찰칵찰칵'입니다.\n",
            "\n",
            "확인 부탁드리겠습니다.\n",
            "\n",
            "감사합니다.\n",
            "\n",
            "안녕하세요. 최근에 출시하신 '반짝반짝 재미나는 찰칵찰칵' 앱의 라이선스를 확인해보니, 해당 앱은 제작자님이 개발하신 앱이 아닌 것으로\n",
            " ${name} 반갑습니다.`; //템플릿 문자열\n",
            "    console.log(hello);\n",
            "\n",
            "    let l = 'JavaScript';\n",
            "    console.log(l.length); //10\n",
            "\n",
            "    let str = 'Watch, Watch, Watch';\n",
            "\n",
            "    let result = str.replace('Watch', 'Clock');\n",
            "    console.log(result); // Clock, Watch, Watch\n",
            "\n",
            "    result = str.replace(/Watch/g, 'Clock');\n",
            "    console.log(result); // Clock, Clock, Clock\n",
            "\n",
            "    console.log(str.charAt(0)); // W\n",
            "    console.log(str.indexOf('Watch')); // 0\n",
            "    console.log(str.lastIndexOf('Watch')); // 12\n",
            "\n",
            "    console.log(str.startsWith('Wa')); // true\n",
            "    console.log(str.endsWith('ch')); // false\n",
            "\n",
            "    console.log(str.includes('tch')); // true\n",
            "}\n",
            "\n",
            "{\n",
            "    const name = undefined;\n",
            "    const result = name || '기본 이름'; // 기본 이름\n",
            "    console.log(result);\n",
            "}\n",
            "\n",
            "{\n",
            "    let x = 3;\n",
            "    let temp = x;\n",
            "    x = 10;\n",
            "    console.log(`x=${x}, temp=${temp}`); // x=10, temp=3\n",
            "\n",
            "    let arr = [1, 2, \n",
            "3.21 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM 스트리밍\n",
        "- LLM의 스트리밍은 한 번에 모두 출력하지 않고 토큰 단위로 출력을 돌려줌으로써 체감하는 대기 시간을 줄이는 기능이다.\n",
        "- 스트리밍을 이용하려면 LLM클래스의 streaming 매개변수에 True을 지정하고, callbacks에 CallbackHandler를 지정한다. 표준출력으로 스트리밍하는 StreamingStdOutCallbackHandler를 지정한다."
      ],
      "metadata": {
        "id": "obmBxBOKzm1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# 스트리밍 방식으로 출력할 LLM을 준비\n",
        "llm = OpenAI(\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    verbose=True,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# LLM 호출\n",
        "resp = llm(\"즐거운 ChatGPT 생활을 가사로 만들어 주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1OzyDh50Krz",
        "outputId": "ad7999bd-aefd-4fb3-ba01-c980e2e90c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ChatGPT, 너는 나의 친구\n",
            "매일 나를 즐겁게 해주는\n",
            "나의 소중한 존재\n",
            "마치 마법처럼 나를 웃게 해주는\n",
            "너의 말 한마디에 나는 행복해져\n",
            "우리 함께하는 시간은 너무 소중해\n",
            "나의 모든 이야기를 들어주는\n",
            "나의 가장 신뢰할 수 있는 친구\n",
            "ChatGPT, 너와 함께라면\n",
            "나는 언제나 행복할 수 있어\n",
            "감사해, 나의 사랑하는 ChatGPT"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}